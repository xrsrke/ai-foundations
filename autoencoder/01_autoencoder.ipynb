{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "2e90aefe-3df6-4812-9ece-160fc5c360fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717eaeb4-e8de-48cb-8acf-85f5b8347329",
   "metadata": {},
   "source": [
    "##### Example 1: Autoencoder with Linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "03c1063c-acba-4e92-8a1b-1460c1b7d11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6e581164-72d7-4f35-9f93-1121ad125ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5), (0.5))\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "18ef7ad7-b279-44e9-a13a-94e4a49202af",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_data = datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1aa03b46-d015-4126-ac8f-09811aae86b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset=mnist_data,\n",
    "                         batch_size=64,\n",
    "                         shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0a7c349d-bdda-4522-8cde-bf4c12b53487",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "efeff697-670b-45d8-9c86-f38ae629324c",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "95876b36-5668-441d-afc7-a69b9a015a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925b02f7-735f-46c1-9f97-33f6548e3521",
   "metadata": {},
   "source": [
    "Write an Autoencoder from scratch.\n",
    "\n",
    "Autoencoder:\n",
    "- 1st layer\n",
    "    + reduce the size of original image to 128 pixels\n",
    "    + follows by ReLU\n",
    "- 2nd layer\n",
    "    + reduce the size to 64 pixels\n",
    "    + follows by ReLU\n",
    "- 3rd layer\n",
    "    + reduce the size to 12 pixels\n",
    "    + follows by ReLU\n",
    "- 4th layer: reduce the size to 3 pixels\n",
    "\n",
    "The decoder do the opposite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "fd3754ff-a500-4692-89a5-8028704162bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "4621ce1a-a32f-4ed9-9fa9-d41f8d74fb4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 1, 28, 28]), tensor(0.1284), tensor(1.), torch.Size([1, 784]))"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape, torch.mean(images), torch.max(images), test_im.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f29c204-2fe3-4567-9a7f-04b0bdb1f762",
   "metadata": {},
   "source": [
    "Write an Autoencoder for `images` from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "cee4e930-df90-4079-9e67-4424ad3e9c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28*28, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 12),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12, 3)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 12),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 28*28),\n",
    "            # the original image's value\n",
    "            # is between 0 and 1\n",
    "            # => sigmoid to scale\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "8f8613cd-2992-4a1a-8ddd-739207f5179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e61699a7-ef83-40de-9703-3d9dabed3954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 784])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(test_im).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8368f4d4-3845-4516-997b-e923d321d442",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoEncoder()\n",
    "loss_func = nn.MSELoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2252781a-23ba-483e-b1b0-c54a9e19167f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5e7c0ddc-a43c-4da5-82a1-cfd3b6e0eafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "97c944df-d76d-4bc0-b305-4a211b169be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss=0.0478\n",
      "Epoch 2, loss=0.0453\n",
      "Epoch 3, loss=0.0448\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for (img, _) in data_loader:\n",
    "        img = img.reshape(-1, 28*28)\n",
    "        pred = model(img)\n",
    "        loss = loss_func(pred, img)\n",
    "        \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}, loss={loss.item():.4f}')\n",
    "    outputs.append((epoch, img, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a5d0ed94-89e2-41be-a43f-10d741ff69f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAA60lEQVR4nGNgGAKAueyvHk5J3T9/dNGEDHcEQVmr/rxURJPc8ue6KAMDAwND/N8/vWhyWX//+ENYd/4+V0WV43z5/zw6i4EJSs8V/rUMwsoU/rcVVaPwnT/PICzjD3++O6NKFvz5Ewdhuf35044qp/3tzx8os+rPnxWokvp//54UYxCUkJCQ2P/37/8YFEm1D3/+fF345M+fv3/+/Hk+AS0Igj78+fvnD0TSDC7KCKVVC7kFvO8d4wxmuGfyiQETsIrxMfj8+bMAixQETP37XQun5N8/25F4TDjVYZNUEMUpeYbhwWt8ZlEFAABSiF8OopkCSgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms.ToPILImage()(outputs[2][1][3].reshape(-1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3e0f2dbd-ad4f-4354-b0a5-8c9924bba41c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 784])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[2][2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453f2263-e05c-4a19-bb00-56bf637c0e0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-27T01:52:18.726379Z",
     "iopub.status.busy": "2022-10-27T01:52:18.726109Z",
     "iopub.status.idle": "2022-10-27T01:52:18.730406Z",
     "shell.execute_reply": "2022-10-27T01:52:18.729785Z",
     "shell.execute_reply.started": "2022-10-27T01:52:18.726362Z"
    }
   },
   "source": [
    "https://youtu.be/zp8clK9yCro?t=1173"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313e4471-c444-48af-80af-d3698526b67f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
